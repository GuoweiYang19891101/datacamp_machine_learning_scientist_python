{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f4ffef9-9ffd-4672-9434-f346c8d4cdad",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boosting with XGBoost\n",
    "\n",
    "#### Chapter 1. Classification with XGBoost\n",
    "\n",
    "##### PART 1.1 Introducing XGBoost\n",
    "\n",
    "What makes XGBoost so popular?\n",
    "\n",
    "1. speed and power\n",
    "2. core algorithm is parallelizable\n",
    "3. consistently outperforms single-algorithm methods\n",
    "4. state-of-art performance in many Machine Learning tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f2653-bb84-413d-a8b7-90c448e65079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# load data\n",
    "churn_data = pd.read_csv('')\n",
    "\n",
    "# create arrays for features and target: X, y\n",
    "X, y = churn_data.iloc[:, :-1], churn_data.iloc[:, -1]\n",
    "\n",
    "# create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=123)\n",
    "\n",
    "# instantiate a XGBClassifier\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                         n_estimators=10,\n",
    "                         seed=123)\n",
    "\n",
    "# fit the classifiter to the training set\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# predict the labels of the test set\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# compute the accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print('Accuracy: %f' %(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da6c897-aaaf-489a-95dc-729a6639d101",
   "metadata": {},
   "source": [
    "##### Part 1.2 Decision Tree\n",
    "\n",
    "Decision tree is XGBoost's base learner. A question is asked on each decision tree node and there are two possible choices on each node. At the bottom of each decision tree, there is a single possible decision. It is constructed iteratively until a stopping criterion is met.\n",
    "\n",
    "Individual decision trees are in general low-bias, high-variance learning models. Thus, XGBoost uses **CART** (classification and regression trees), each leaf always contains a real-value score, and it can be converted into categories if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907adcad-9978-47d8-a166-0930e4c64aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# load data\n",
    "breast_cancer = pd.read_csv('')\n",
    "\n",
    "# create arrays for the features and target: X, y\n",
    "X, y = breast_cancer.iloc[:, :-1], breast_cancer.iloc[:, -1]\n",
    "\n",
    "# create  the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=123)\n",
    "\n",
    "# instantiate the classifier\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# fit the classifier\n",
    "dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# predict the labels of test set\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# compute the accuracy\n",
    "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "print('Accuracy: %f' %(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c62107-4120-4623-9ec3-58acc3c96e7d",
   "metadata": {},
   "source": [
    "##### PART 1.3 Boosting\n",
    "\n",
    "Boosting is not a specific meachine learning algorithm, it is a concept that can be applied to a set of machine learning models, it can be called a **\"meta-algorithm\"**.\n",
    "\n",
    "In short, boosting can convert a collection of weak learners into a stronger learner.\n",
    "\n",
    "How it works?\n",
    "\n",
    "1. Iteratively learning a set of weak models on subsets of the data\n",
    "2. Weigning each weak prediction according to each weak learner's performance\n",
    "3. Combine the weighed predictions to obtain a single weighted prediction\n",
    "4. Then the result is much better than the individual predictions themselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144093c4-92ac-4e37-bf18-314dfb06a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load data\n",
    "churn_data = pd.read_csv('')\n",
    "\n",
    "# create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:, :-1], churn_data.iloc[:, -1]\n",
    "\n",
    "# create the DMatrix from X and y\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# create the parameter dictionary for cross-validation\n",
    "params = {'objective': 'reg:logistic', 'max_depth':3}\n",
    "\n",
    "# perform cross-validation\n",
    "cv_results = xgb.cv(params=params,\n",
    "                    dtrain=churn_dmatrix, nfold=3, \n",
    "                    num_boost_round=5, metrics='error',\n",
    "                    as_pandas=True, seed=123)\n",
    "print(cv_results)\n",
    "\n",
    "# print the accuracy\n",
    "print((1-cv_results['test-error-mean']).iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1915582d-eca5-4344-be6a-447cf18343f5",
   "metadata": {},
   "source": [
    "##### Part 1.4 When should I use XGBoost?\n",
    "\n",
    "When to use:\n",
    "\n",
    "1. You have large number of training samples (> 1000)\n",
    "2. You have a mixture of categorical and numerical features or just numerical features\n",
    "\n",
    "When not to use:\n",
    "\n",
    "1. Image recognition\n",
    "2. Computer vision\n",
    "3. Natural Language Processing (NLP) and understanding problems\n",
    "4. When the number of training samples is smaller than the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e7fbf-50ac-419a-88fa-263761e1b0d6",
   "metadata": {},
   "source": [
    "#### Chapter 2. Regression with XGBoost\n",
    "\n",
    "##### Part 2.1 Regression review\n",
    "\n",
    "What is a regression problem?\n",
    "The outcome is real-valued.\n",
    "\n",
    "Common regression metrics:\n",
    "Root mean squared error (RMSE)\n",
    "Mean absolute error  (MAE)\n",
    "\n",
    "Loss functions and base learners:\n",
    "Loss function quantifies how far off a prediction is from the actual result. Our goal is to minimize the loss function of all of the data points we pass in.\n",
    "\n",
    "The loss function names in XGBoost:\n",
    "1. reg:linear -> use for regressin problems\n",
    "2. reg:logistic -> use for classification problems when you want just decision\n",
    "3. binary:logistic -> use for classification problems when you want probability rather than decision.\n",
    "\n",
    "Baes learners are learners that are slightly better than random guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e02721-8ede-4763-89f2-10351fa66f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1.\n",
    "# load data\n",
    "df = pd.read_csv('')\n",
    "\n",
    "# create features and target: X, y\n",
    "X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "\n",
    "# create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=123)\n",
    "\n",
    "# instantiate the XGBRegressor\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:linear', \n",
    "                          n_estimators=10, \n",
    "                          random_state=123)\n",
    "\n",
    "# fit the classifier\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# predict the labels of the test set\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# compute the rmse\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print('RMSE: %f' %(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cbc5c8-be80-42af-bd55-f84eff6bfdae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9253dafe-c03e-41a4-9fb8-c51da6b580fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
